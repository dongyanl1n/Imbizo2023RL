{"cells":[{"cell_type":"code","source":["#@title Define LinearTrack and GridWorld\n","# =====================================\n","#           IMPORT MODULES            #\n","# =====================================\n","import numpy as np\n","\n","import matplotlib.pyplot as plt\n","import matplotlib.patches as patches\n","\n","# =====================================\n","#              GW CLASS               #\n","# =====================================\n","class GridWorld(object):\n","    def __init__(self, rows=20, cols=20, rewards={(5,5):10},\n","                 env_type=None, rho=0.0, step_penalization=-0.01,\n","                 terminals=None, obstacles=None, jumps=None,\n","                 port_shift=None, barheight=None,\n","                 around_reward=False,\n","                 **kwargs):\n","        '''\n","        :param rows (int): specifying width (x dim) of grid\n","        :param cols (int): specifying height (y dim) of grid\n","        :param rewards (dict): reward location coordinates as keys and reward magnitude as values\n","\n","        :param env_type (str): [None, 'bar','room','tmaze', 'triple_reward']\n","        :param rho (float): density of obstacles to be randomly positioned in environment\n","        :param step_penalization (float): penalization for each step (small negative reward)\n","\n","        :param terminals (list): terminal state coordinates\n","        :param obstacles (list): obstacle coordinates\n","        :param jumps (dict): keys and values are coordinates in the environment (x,y):(w,z)\n","                             where agent can select jump action in coordinate (x,y) and moves to coordinate (w,z)\n","\n","        :param port_shift (str):  ['equal', 'left', 'right']: type of probability distribution for Kirth's task\n","        :param barheight (int): height of horizontal line of obstacles dividing the plane\n","\n","        :param around_reward (bool): whether to randomly choose agent starting location around reward location\n","        '''\n","        self.shape = (rows, cols)\n","        self.c = cols\n","        self.r = rows\n","\n","        self.maze_type = env_type\n","        self.rho = rho\n","\n","        # basic set up states, rewards\n","        self.nstates = self.c*self.r\n","        self.rewards = rewards\n","\n","        # populate list of obstacles\n","        self.obstacles_list = obstacles\n","\n","        if self.maze_type == 'bar':\n","            if barheight is not None:\n","                self.barheight \t= barheight\n","        self.grid, self.useable, self.obstacles_list = self.buildGrid()\n","\n","        # populate list of terminals\n","        if terminals is not None:\n","            if isinstance(terminals, tuple):\n","                self.terminal2D = [terminals]\n","                self.terminal = [self.twoD2oneD(terminals)]\n","            else:\n","                self.terminal2D = terminals\n","                self.terminal = [self.twoD2oneD((r,c)) for r,c in terminals]\n","        else:\n","            self.terminal2D = []\n","            self.terminal = []\n","\n","        # populate list of jump states\n","        if jumps is not None:\n","            self.jump = jumps\n","            self.jump_from = [self.twoD2oneD((r,c)) for r,c in list(jumps.keys())]\n","            self.jump_to = [self.twoD2oneD((r,c)) for r,c in list(jumps.values())]\n","        else:\n","            self.jump = None\n","            self.jump_from = []\n","            self.jump_to = []\n","\n","        # Actions\n","        self.action_list    = kwargs.get('actionlist', ['Down', 'Up', 'Right', 'Left', 'Jump', 'Poke'])\n","        self.action_dict    = kwargs.get('actiondict', {x: ind for ind, x in enumerate([x[0] for x in self.action_list])})\n","        self.nactions       = len(self.action_list)\n","        self.buildTransitionMatrix()\n","\n","        # Reward\n","        self.finish_after_first_reward = kwargs.get('finish_after_first_reward',True)\n","        if self.finish_after_first_reward:\n","            # add reward location to list of terminal states\n","            for r,c in self.rewards.keys():\n","                self.terminal2D.append((r,c))\n","                self.terminal.append(self.twoD2oneD((r, c)))\n","        self.rwd_action = kwargs.get('rewarded_action', 'Poke')\n","        self.step_penalization = step_penalization\n","        self.buildRewardFunction()\n","\n","        # additional features from specific environment types\n","        if self.maze_type == 'tmaze':\n","            if port_shift is not None:\n","                self.port_shift\t= port_shift\n","            else:\n","                self.port_shift = 'none'\n","        if self.maze_type == 'triple_reward':\n","            self.rwd_loc \t\t= [(self.r-1, 0), (self.r-1, self.c-1), (0, self.c-1)]\n","            self.orig_rwd_loc \t= [(self.r-1, 0), (self.r-1, self.c-1), (0, self.c-1)]\n","            self.starter \t\t= kwargs.get('t_r_start', (0,0))\n","            self.start_loc      = self.starter\n","\n","        self.random_start = kwargs.get('random_start',True)\n","        self.reset()\n","        \n","        self.view = True\n","        if self.view:\n","            view_labels = kwargs.get('view_labels',False)\n","            self.reset_viewer(states=view_labels)\n","            self.viewer = self.figure[0].canvas\n","\n","    def oneD2twoD(self, idx):\n","        return (int(idx / self.shape[1]),np.mod(idx,self.shape[1]))\n","\n","    def twoD2oneD(self, coord_tuple):\n","        r,c = coord_tuple\n","        return (r * self.shape[1]) + c\n","\n","    def buildGrid(self, bound=False): #formerly grid_maker()\n","        env_types = [None, 'bar','room','tmaze', 'triple_reward']\n","\n","        if self.maze_type not in env_types:\n","            raise Exception(f\"Environment Type '{self.maze_type}' Not Recognized. \\nOptions are: {env_types} \\nDefault is Open Field (maze_type = 'none')\")\n","\n","        grid = np.zeros((self.r,self.c), dtype=int)\n","\n","        # set up obstables for different grid types\n","        if self.maze_type == 'bar':\n","            space = 2\n","            self.rho = 0\n","            for i in range(self.c-(2*space)):\n","                grid[self.barheight][i+space] = 1\n","\n","        elif self.maze_type == 'room':\n","            self.rho = 0\n","            vwall = int(self.c/2)\n","            hwall = int(self.r/2)\n","\n","            #make walls\n","            for i in range(self.c):\n","                grid[vwall][i] = 1\n","            for i in range(self.r):\n","                grid[i][hwall] = 1\n","\n","            # make doors\n","            self.doors = []\n","            self.doors.append((vwall, np.random.choice(np.arange(0,vwall))))\n","            self.doors.append((vwall, np.random.choice(np.arange(vwall+1, self.c))))\n","\n","            self.doors.append((np.random.choice(np.arange(0,hwall)), hwall))\n","            self.doors.append((np.random.choice(np.arange(hwall+1, self.r)),hwall))\n","            for i in self.doors:\n","                grid[i] = 0\n","\n","        elif self.maze_type == 'tmaze':\n","            self.rho = 0\n","            self.possible_ports = []\n","            grid = np.ones((self.r, self.c), dtype=int)\n","            h1, v1 = int(self.c/2), 0\n","            if h1%2==0:\n","                for i in range(self.c):\n","                    grid[v1][i] = 0\n","                    if i == 0:\n","                        self.possible_ports.append((i,v1))\n","                    elif i == self.c-1:\n","                        self.possible_ports.append((i,v1))\n","            else:\n","                for i in range(self.c):\n","                    grid[v1][i] = 0\n","                    if i == 0:\n","                        self.possible_ports.append((i,v1))\n","                    elif i == self.c-1:\n","                        self.possible_ports.append((i,v1))\n","\n","            if self.r > int(self.c/2):\n","                for i in range(self.r):\n","                    grid[i][h1] = 0\n","                    if i == self.r-1:\n","                        self.possible_ports.append((h1,i))\n","            else:\n","                for i in range(self.r):\n","                    grid[i][h1] = 0\n","                    if i == self.r-1:\n","                        self.possible_ports.append((h1,i))\n","\n","        if self.obstacles_list is None:\n","            if self.rho != 0:\n","                maze = np.vstack([[np.random.choice([0,1], p = [1-self.rho, self.rho]) for _ in range(self.c)] for _ in range(self.r)])\n","                grid = grid + maze\n","                for reward_loc in self.rewards:\n","                    if grid[reward_loc[1], reward_loc[0]] == 1:\n","                        grid[reward_loc[1], reward_loc[0]] = 0\n","\n","            obstacles = list(zip(np.where(grid==1)[0], np.where(grid==1)[1]))\n","            self.obstacle2D = obstacles\n","            self.obstacle = [self.twoD2oneD((r,c)) for r,c in obstacles]\n","\n","        else:\n","            for reward_loc in self.rewards:\n","                if reward_loc in self.obstacles_list:\n","                    self.obstacles_list.remove(reward_loc)\n","            if isinstance(self.obstacles_list, tuple):\n","                self.obstacle2D = [self.obstacles_list]\n","                self.obstacle = [self.twoD2oneD(self.obstacles_list)]\n","            else:\n","                self.obstacle2D = self.obstacles_list\n","                self.obstacle = [self.twoD2oneD((r,c)) for r,c in self.obstacles_list]\n","                for coord in self.obstacles_list:\n","                    grid[coord] = 1\n","\n","        if bound:\n","            grid_bound = np.ones((self.r+2, self.c+2), dtype=int)\n","            grid_bound[1:-1][:,1:-1] = grid\n","            grid = grid_bound\n","\n","        # lists of tuples storing locations of open grid space and obstacles (unusable grid space)\n","        useable_grid = list(zip(np.where(grid==0)[0], np.where(grid==0)[1]))\n","        obstacles = list(zip(np.where(grid==1)[0], np.where(grid==1)[1]))\n","\n","        return grid, useable_grid, obstacles\n","\n","    def buildRewardFunction(self):\n","        if self.rwd_action in self.action_list:\n","            self.R = self.step_penalization*np.ones((self.nstates, len(self.action_list)))\n","            action = self.action_list.index(self.rwd_action)\n","            for r,c in list(self.rewards.keys()):\n","                self.R[self.twoD2oneD((r,c)), action] = self.rewards[(r,c)]\n","        else:\n","            # specify reward function\n","            self.R = self.step_penalization*np.ones((self.nstates,))  # rewards received upon leaving state\n","            for r,c in list(self.rewards.keys()):\n","                self.R[self.twoD2oneD((r,c))] = self.rewards[(r,c)]\n","\n","    def buildTransitionMatrix(self):\n","        # initialize\n","        self.P = np.zeros((len(self.action_list), self.nstates, self.nstates))  # down, up, right, left, jump, poke\n","\n","        # add neighbor connections and jumps, remove for endlines\n","        self.P[0, list(range(0, self.nstates-self.shape[1])), list(range(self.shape[1], self.nstates))] = 1     # down\n","        self.P[1, list(range(self.shape[1], self.nstates)), list(range(0, self.nstates-self.shape[1]))] = 1  \t# up\n","\n","        self.P[2, list(range(0, self.nstates-1)), list(range(1, self.nstates))] = 1  \t\t\t\t\t\t\t# right\n","        self.P[3, list(range(1, self.nstates)), list(range(0, self.nstates-1))] = 1  \t\t\t\t\t\t\t# left\n","        if len(self.action_list) > 4:\n","            self.P[4, self.jump_from, self.jump_to] = 1\t\t\t\t\t\t\t\t\t\t\t\t# jump\n","\n","        # remove select states\n","        endlines = list(range(self.shape[1]-1,self.nstates-self.shape[1],self.shape[1]))\n","        endlines2 = [x+1 for x in endlines]\n","        self.P[2, endlines, endlines2] = 0\t# remove transitions at the end of the grid\n","        self.P[3, endlines2, endlines] = 0\n","        for i in range(4):\n","            self.P[i, :, self.obstacle] = 0  \t# remove transitions into obstacles\n","            self.P[i, self.obstacle, :] = 0  \t# remove transitions from obstacles\n","            self.P[i, self.terminal, :] = 0  \t# remove transitions from terminal states\n","            if len(self.action_list) >4:\n","                self.P[i, self.jump_from, :] = 0 \t# remove neighbor transitions from jump states\n","\n","        if len(self.action_list) >5:\n","            # poke should make no transitions between states so everything stays 0\n","            self.P[5, list(range(0, self.nstates)), list(range(0, self.nstates))] = 1\n","\n","    def remapTransitionMatrix(self):\n","        oldP = self.P\n","\n","        # initalize\n","        self.P = np.zeros((len(self.action_list), self.nstates, self.nstates))  # down, up, right, left, jump, poke\n","\n","        for x in range(oldP.shape[0]):\n","            col = (x + 1) % oldP.shape[0]\n","            self.P[col, :, :] = oldP[x, :, :]\n","\n","        print(\"transition probabilities remapped\")\n","\n","    def get_random_start_location(self):\n","        get_start = np.random.choice(len(self.useable))\n","        start_r = self.useable[get_start][0]\n","        start_c = self.useable[get_start][1]\n","        return (start_r, start_c)\n","\n","    def get_start_location(self, around_reward, **kwargs):\n","        if around_reward:\n","            radius = kwargs.get('rad', 5)\n","            # pick starting location for agent in radius around reward location\n","            start_buffer = radius  # radius around reward\n","            starting_reward = list(self.rewards.keys())[np.random.choice(np.arange(len(self.rewards.keys())))]\n","            get_start_loc = True\n","            while get_start_loc:\n","                buf_r = np.random.choice(np.arange(start_buffer))\n","                start_r = starting_reward[0] + np.random.choice([-1, 1])*buf_r\n","                if start_r < 0:\n","                    start_r = 0\n","                elif start_r > self.grid.shape[0] - 1:\n","                    start_r = self.grid.shape[0] - 1\n","\n","                buf_c = np.random.choice(np.arange(start_buffer))\n","                start_c = starting_reward[1] + np.random.choice([-1, 1])*buf_c\n","                if start_c < 0:\n","                    start_c = 0\n","                elif start_c > self.grid.shape[1] - 1:\n","                    start_c = self.grid.shape[1] - 1\n","                if (start_r, start_c) in self.useable:\n","                    get_start_loc = False\n","        else: # pick a random starting location for agent within the useable spaces\n","            get_start = np.random.choice(len(self.useable))\n","            start_r = self.useable[get_start][0]\n","            start_c = self.useable[get_start][1]\n","\n","        start_coord = (start_r, start_c)\n","\n","        return start_coord\n","\n","    def set_state(self, state):\n","        self.state = state\n","\n","    def get_state(self):\n","        return self.state\n","\n","    def get_actions(self):\n","        slice_n_dice = self.P[:,self.state,:]\n","        return np.any(slice_n_dice,axis=1)\n","\n","    def set_reward(self, rewards):\n","        self.rewards = rewards\n","\n","        # recalculate reward function\n","        self.buildRewardFunction()\n","\n","    def get_reward(self, action):\n","        action = self.action_list[action][0]\n","        if len(self.R.shape) > 1:\n","            reward = self.R[self.state,self.action_dict[action]]\n","        else:\n","            reward = self.R[self.state]\n","        if self.finish_after_first_reward and reward in self.rewards.values():\n","            self.done = True\n","        \n","        # check if this is a terminal state\n","        if self.state in self.terminal:\n","            self.done = True\n","\n","        # TODO: fix for kirth\n","        if self.maze_type == 'tmaze':\n","            if self.port_shift in ['equal', 'left', 'right']:\n","                self.shift_rwd(self.port_shift)\n","\n","        return reward\n","    \n","    def reset_viewer(self, **kwargs):\n","        trial = kwargs.get('trial', 'Grid World')\n","        states = kwargs.get('states',False)\n","        self.figure = plot_world(self, title=f'Trial {trial}',states=states)\n","        ## test\n","        fig, ax = self.figure\n","        agent_r, agent_c = self.oneD2twoD(self.state)\n","        patch = patches.Circle((agent_c + .5, agent_r + .5), 0.35,\n","                               fc='b')  ## plot functions use x,y we use row(y), col(x)\n","        ax.add_patch(patch)\n","        fig.canvas.draw()\n","        plt.show(block=False)\n","        ## /test\n","        \n","    def render(self, pause_time=0.01, mode='human', **kwargs):\n","        trial = kwargs.get('trial', None)\n","        if mode == 'human':\n","            agent_r, agent_c = self.oneD2twoD(self.state)\n","            self.figure[1].patches[1].set_center([agent_r + 0.5, agent_c + 0.5])\n","            self.figure[0].canvas.draw()\n","            plt.pause(pause_time)\n","\n","            # TODO: fix this so render just updates the current_state patch\n","            # TODO: base object to write current_state patch on top of\n","        else:\n","            assert 0, \"Render mode '%s' is not supported\" %mode\n","\n","    def close(self):\n","        if self.viewer is not None:\n","            self.viewer.close()\n","            self.viewer = None\n","\n","    def reset(self):\n","        if self.random_start:\n","            self.start = self.get_random_start_location()\n","        else:\n","            self.start = self.useable[0]\n","        self.state = self.twoD2oneD(self.start)\n","\n","        self.done = False\n","\n","        return self.state\n","\n","    def step(self, action):\n","        \"\"\"\n","        Args:\n","            move (str): one of ['D','U','R','L','J'] for down, up, right, left, and jump, respectively.\n","        Returns:\n","            tuple (a,b,c,d): a is the new state, b is the reward value, and c is a bool signifying terminal state, d is an empty dictionary to conform with OpenAi gym step function returning an 'info' parameter\n","        \"\"\"\n","\n","        # check if move is valid, and then move\n","        x = self.get_actions()\n","        if not self.get_actions()[action]:\n","            #raise Exception('Agent has tried an invalid action!')\n","            pass\n","        else:\n","            transition_probs = self.P[action, self.state,:]\n","            self.state = np.nonzero(transition_probs)[0][0]  # update to new state\n","\n","        reward = self.get_reward(action) ## self.done is set in this function\n","\n","        is_terminal = self.done\n","\n","        return self.state, reward, is_terminal, {}\n","\n","class GW_open_field(GridWorld):\n","    def __init__(self):\n","        self.action_list = ['Down', 'Up', 'Right', 'Left']\n","        self.rewarded_action = None\n","        super().__init__(actionlist=self.action_list, rewarded_action=self.rewarded_action)\n","\n","    \n","class GridWorld_random_obstacle(GridWorld):\n","    def __init__(self):\n","        self.action_list = ['Down', 'Up', 'Right', 'Left']\n","        self.rewarded_action = None\n","        # obstacles list corresponds to one instance of rho = 0.1\n","        # using list of obstacles instead so that they are the same each instantiation\n","        # using rho generates new obstacles each time\n","        self.obstacles_list = [(0, 11), (0, 14), (3, 1), (3, 19), (4, 4), (4, 11), (4, 15), (4, 17), (6, 4), (6, 18), (7, 6), (8, 1), (8, 11), (9, 0), (9, 8), (9, 14), (10, 13), (11, 4), (11, 16), (12, 5), (12, 18), (12, 19), (13, 2), (13, 5), (13, 15), (14, 6), (14, 9), (14, 19), (15, 4), (15, 7), (15, 19), (16, 7), (17, 0), (17, 2), (17, 11), (18, 1), (19, 5), (19, 7), (19, 11)]\n","        super().__init__(actionlist=self.action_list, rewarded_action=self.rewarded_action,obstacles=self.obstacles_list)\n","\n","\n","class GridWorld_4rooms(GridWorld):\n","    def __init__(self):\n","        self.action_list = ['Down', 'Up', 'Right', 'Left']\n","        self.rewarded_action = None\n","        self.obstacles_list = [(0, 10), (1, 10), (2, 10), (3, 10), (5, 10), (6, 10), (7, 10), (8, 10), (9, 10), (10, 0), (10, 1), (10, 3), (10, 4), (10, 5), (10, 6), (10, 7), (10, 8), (10, 9), (10, 10), (10, 11), (10, 12), (10, 13), (10, 14), (10, 16), (10, 17), (10, 18), (10, 19), (11, 10), (12, 10), (14, 10), (15, 10), (16, 10), (17, 10), (18, 10), (19, 10)]\n","        super().__init__(actionlist=self.action_list, rewarded_action=self.rewarded_action,obstacles=self.obstacles_list)\n","\n","\n","\n","class GridWorld_bar(GridWorld):\n","    def __init__(self):\n","        self.action_list = ['Down', 'Up', 'Right', 'Left']\n","        self.rewarded_action = None\n","        self.maze_type = 'bar'\n","        self.barheight = 9\n","        super().__init__(actionlist=self.action_list, rewarded_action=self.rewarded_action, env_type=self.maze_type, barheight=self.barheight)\n","\n","class GridWorld_tunnel(GridWorld):\n","    def __init__(self):\n","        self.action_list = ['Down', 'Up', 'Right', 'Left']\n","        self.rewarded_action = None\n","        # obstacles list corresponds to one instance of rho = 0.1\n","        # using list of obstacles instead so that they are the same each instantiation\n","        # using rho generates new obstacles each time\n","        tunnel_blocks = []\n","        for i in range(20):\n","            for j in range(7,13):\n","                if i == 9:\n","                    pass\n","                else:\n","                    tunnel_blocks.append((i,j))\n","\n","        self.obstacles_list = tunnel_blocks\n","        self.rewards = {(5,5):10}\n","        super().__init__(cols=20,rows=20,actionlist=self.action_list, rewards=self.rewards, rewarded_action=self.rewarded_action, obstacles=self.obstacles_list)\n","\n","class GridWorld_hairpin(GridWorld):\n","    def __init__(self):\n","        self.action_list = ['Down', 'Up', 'Right', 'Left']\n","        self.rewarded_action = None\n","        # obstacles list corresponds to one instance of rho = 0.1\n","        # using list of obstacles instead so that they are the same each instantiation\n","        # using rho generates new obstacles each time\n","        hairpin_blocks = []\n","        ncols = 19\n","        nrows = 20\n","        for i in range(ncols):\n","            for j in range(nrows):\n","                if i%2==0:\n","                    pass\n","                else:\n","                    hairpin_blocks.append((j,i))\n","        for i in range(ncols):\n","            if i%2==0:\n","                pass\n","            elif i%4==1:\n","                hairpin_blocks.remove((nrows-1,i))\n","            elif i%4==3:\n","                hairpin_blocks.remove((0, i))\n","\n","        self.obstacles_list = hairpin_blocks\n","        self.rewards = {(0,0):10}\n","        super().__init__(cols=ncols,rows=nrows, actionlist=self.action_list, rewards=self.rewards,\n","                         rewarded_action=self.rewarded_action, obstacles=self.obstacles_list, random_start=True)\n","\n","\n","class LinearTrack(GridWorld):\n","    def __init__(self):\n","        self.action_list = ['Right', 'Left']\n","        self.rewarded_action = None\n","        track_length = 20\n","        self.rewards = {(0,track_length-1):10}\n","        super().__init__(rows=1, cols=track_length, actionlist=self.action_list, rewarded_action=self.rewarded_action, rewards=self.rewards)\n","\n","    def buildTransitionMatrix(self):\n","        # initialize\n","        self.P = np.zeros((len(self.action_list), self.nstates, self.nstates))  # down, up, right, left, jump, poke\n","\n","        self.P[0, list(range(0, self.nstates-1)), list(range(1, self.nstates))] = 1  \t\t\t\t\t\t\t# right\n","        self.P[1, list(range(1, self.nstates)), list(range(0, self.nstates-1))] = 1  \t\t\t\t\t\t\t# left\n","\n","\n","def plot_world(world, **kwargs):\n","    scale = kwargs.get('scale', 0.35)\n","    title = kwargs.get('title', 'Grid World')\n","    ax_labels = kwargs.get('ax_labels', False)\n","    state_labels = kwargs.get('states', False)\n","    invert_ = kwargs.get('invert', False)\n","    if invert_:\n","        cmap = 'bone'\n","    else:\n","        cmap = 'bone_r'\n","    r,c = world.shape\n","\n","    fig = plt.figure(figsize=(c*scale, r*scale))\n","    ax = fig.add_subplot(1,1,1)\n","\n","    gridMat = np.zeros(world.shape)\n","    for i, j in world.obstacle2D:\n","        gridMat[i, j] = 1.0\n","    for i, j in world.terminal2D:\n","        gridMat[i, j] = 0.2\n","    ax.pcolor(gridMat, edgecolors='k', linewidths=0.75, cmap=cmap, vmin=0, vmax=1)\n","\n","    U = np.zeros((r, c))\n","    V = np.zeros((r, c))\n","    U[:] = np.nan\n","    V[:] = np.nan\n","\n","    if len(world.action_list) >4 :\n","        if world.jump is not None:\n","            for (a, b) in world.jump.keys():\n","                (a2, b2) = world.jump[(a, b)]\n","                U[a, b] = (b2 - b)\n","                V[a, b] = (a - a2)\n","\n","    C, R = np.meshgrid(np.arange(0, c) + 0.5, np.arange(0, r) + 0.5)\n","    ax.quiver(C, R, U, V, scale=1, units='xy')\n","\n","    for rwd_loc in world.rewards.keys():\n","        rwd_r, rwd_c = rwd_loc\n","        if world.rewards[rwd_loc] < 0:\n","            colorcode = 'red'\n","        else:\n","            colorcode = 'darkgreen'\n","        ax.add_patch(plt.Rectangle((rwd_c, rwd_r), width=1, height=1, linewidth=2, facecolor=colorcode, alpha=0.5))\n","\n","    if state_labels:\n","        for (i,j) in world.useable:\n","            # i = row, j = col\n","            ax.text(j+0.5,i+0.7, s=f'{world.twoD2oneD((i,j))}', ha='center')\n","\n","\n","    #ax.set_xticks([np.arange(c) + 0.5, np.arange(c)])\n","    #ax.set_yticks([np.arange(r) + 0.5, np.arange(r)])\n","    ax.invert_yaxis()\n","    ax.set_aspect('equal')\n","    if not ax_labels:\n","        ax.get_xaxis().set_ticks([])\n","        ax.get_yaxis().set_ticks([])\n","    ax.set_title(title)\n","\n","    return fig, ax"],"metadata":{"cellView":"form","id":"kggZbXS_8Axn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Define helper functions\n","\n","from tabulate import tabulate\n","def discount_rwds(r, gamma = 0.99):\n","    disc_rwds = np.zeros_like(r)\n","    running_add = 0\n","    for t in reversed(range(0, len(r))):\n","        running_add = running_add*gamma + r[t]\n","        disc_rwds[t] = running_add\n","    return disc_rwds\n","\n","def running_mean(x, N):\n","    cumsum = np.cumsum(np.insert(x, 0, 0))\n","    return (cumsum[N:] - cumsum[:-N]) / float(N)\n","\n","\n","def oneD2twoD(env_shape, idx):\n","    return (int(idx / env_shape[1]),np.mod(idx,env_shape[1]))\n","\n","def twoD2oneD(env_shape, coord_tuple):\n","    r,c = coord_tuple\n","    return (r * env_shape[1]) + c\n","\n","def sample_MC_trajectory(S, P):\n","    '''\n","    S (list) : state indicies\n","    P (array): transition probability matrix\n","    '''\n","    trajectory = []\n","    done = False\n","    state_list = [\"C1\", \"C2\", \"C3\", \"Pass\", \"Pub\", \"FB\", \"Sleep\"]\n","    terminal_states = [\"Sleep\", \"Pass\"]\n","    terminal_indices = [state_list.index(x) for x in terminal_states]\n","    \n","    # pick first state randomly (not including terminal states)\n","    state = np.random.choice(np.delete(S, terminal_indices))\n","    trajectory.append(state_list[state])\n","    \n","    while not done:\n","        # get next states from distribution given by\n","        # the row of P corresponding to current state\n","        state = np.random.choice(S, 1, p = P[state])[0]\n","        trajectory.append(state_list[state])\n","        # if in terminal state, finish trajectory\n","        if state == state_list.index(\"Sleep\"): \n","            done = True\n","\n","    return trajectory\n","\n","def sample_MRP_trajectory(S, P, R):\n","    '''\n","    S (list) : state indicies\n","    P (array): transition probability matrix\n","    R (array) : reward vector\n","    '''\n","\n","    trajectory = []\n","    rewards = [] \n","    done = False\n","    state_list = [\"C1\", \"C2\", \"C3\", \"Pass\", \"Pub\", \"FB\", \"Sleep\"]\n","\n","    terminal_states = [\"Sleep\", \"Pass\"]\n","    terminal_indices = [state_list.index(x) for x in terminal_states]\n","    \n","    # pick first state randomly (not including terminal states)\n","    state = np.random.choice(np.delete(S, terminal_indices))\n","    trajectory.append(state_list[state])\n","    rewards.append(R[state])\n","    while not done:\n","        # get next states from distribution given by\n","        # the row of P corresponding to current state\n","        state = np.random.choice(S, 1, p = P[state])[0]\n","        trajectory.append(state_list[state])\n","        rewards.append(R[state])\n","        # if in terminal state, finish trajectory\n","        if state == state_list.index(\"Sleep\"): \n","            done = True\n","\n","    return trajectory, rewards\n","\n","def sample_MDP_trajectory(S, P, R, start_state, action_sequence=None):\n","    '''\n","    P (array)              : transition probability matrix\n","    R (list)               : reward vector\n","    start_state (str)      : state in the state list\n","    action_sequence (list) : list of numbers corresponding to actions taken at each step \n","    '''\n","\n","    trajectory = []\n","    rewards = [] \n","    done = False\n","    state_list = [\"C1\", \"C2\", \"C3\", \"Pass\", \"Pub\", \"FB\", \"Sleep\"]\n","    action_list = ['Chill', 'Study']\n","    # check state is valid \n","    if not start_state in state_list:\n","        raise Exception('Agent has tried an invalid action!')\n","    \n","    # pick first state randomly (not including terminal states)\n","    state = state_list.index(start_state)\n","    trajectory.append(state_list[state])\n","    rewards.append(R[state])\n","    \n","    if action_sequence is not None:\n","        action_seq = []\n","        for action in action_sequence:\n","            # get next states from distribution given by\n","            # the row of P corresponding to current state\n","            state = np.random.choice(S, 1, p = P[action,state])[0]\n","            trajectory.append(state_list[state])\n","            action_seq.append(action_list[action])\n","            rewards.append(R[state])\n","            # if in terminal state, finish trajectory\n","            if state == state_list.index(\"Sleep\"): \n","                done = True\n","    else:\n","        ## use a random policy for action selection\n","        action_seq = []\n","        while not done:\n","            action = np.random.choice([0,1])\n","            state = np.random.choice(S, 1, p = P[action,state])[0]\n","            trajectory.append(state_list[state])\n","            action_seq.append(action_list[action])\n","            rewards.append(R[state])\n","            # if in terminal state, finish trajectory\n","            if state == state_list.index(\"Sleep\"): \n","                done = True\n","\n","    return trajectory, action_seq, rewards\n","\n","# backward rollout of return value through the reward vector\n","def discount_rwds(rewards, gamma):\n","    rewards = np.asarray(rewards)\n","    disc_rwds = np.zeros_like(rewards)\n","    running_add = 0\n","    for t in reversed(range(0, rewards.size)):\n","        running_add = running_add*gamma + rewards[t]\n","        disc_rwds[t] = running_add\n","    return disc_rwds\n","\n","# Show computation of return for t= 0\n","def first_element_return(T, rewards, gamma):\n","    '''\n","    T (list)        : trajectory of states visited\n","    rewards (array) : rewards received along trajectory\n","    gamma (float)   : discount factor\n","    '''\n","    discount_powers = []\n","    for i in range(len(T)):\n","        discount_powers.append(gamma**i)\n","    \n","    comp1 = f\"({discount_powers[0]})({rewards[0]})\"\n","    for i in range(len(T)-1):\n","        comp1 += f\" + ({discount_powers[i+1]})({rewards[i+1]})\"\n","    print(f\"\\nG_0 ({T[0]} at t=0):\\n={comp1}\")\n","    \n","    comp2 = f\"{discount_powers[0] * rewards[0]}\"\n","    for i in range(len(T)-1):\n","        comp2 += f\" + {discount_powers[i+1] * rewards[i+1]}\"\n","    \n","    print(f\"= {comp2}\")\n","    \n","    comp3 = sum([x*y for x,y in zip(discount_powers, rewards)])\n","    print(f\"= {comp3}\")\n","\n","def get_MRP_values(S, P, R, gamma, num_runs):\n","    '''\n","    S (list) : state indicies\n","    P (array): transition probability matrix\n","    R (list) : reward vector\n","    '''\n","    state_list = [\"C1\", \"C2\", \"C3\", \"Pass\", \"Pub\", \"FB\", \"Sleep\"]\n","    avgd_st_vals = []\n","    for j in range(len(state_list)):\n","        get_state_values = []\n","        for i in range(num_runs):\n","            trajectory = []\n","            rewards = [] \n","            done = False\n","\n","            # start all of your trajectories in jth state \n","            state = j\n","            trajectory.append(state_list[state])\n","            rewards.append(R[state])\n","            while not done:\n","                # get next states from distribution given by\n","                # the row of P corresponding to current state\n","                state = np.random.choice(S, 1, p = P[state])[0]\n","                trajectory.append(state_list[state])\n","                rewards.append(R[state])\n","                # if in terminal state, finish trajectory\n","                if state == state_list.index(\"Sleep\"): \n","                    done = True\n","\n","            get_state_values.append(discount_rwds(rewards, gamma)[0])\n","        avgd_st_vals.append(np.round(np.mean(get_state_values),4))\n","    avgd_st_vals_dict = dict(zip(state_list,avgd_st_vals))\n","    return avgd_st_vals_dict\n","\n","\n","def show_trajectory_table(T, rewards, gammas, actions=None):\n","    '''\n","    T (list)        : states visited along trajectory\n","    rewards (array) : rewards collected along trajectory\n","    gammas (list)   : discount factors to compute return values\n","    '''\n","    if actions==None:\n","        data = {'state':T, 'reward':rewards}\n","        headers = [\"Step\",\"Reward\"]\n","    else:\n","        \n","        data = {'state':T,'actions':actions, 'reward':rewards}\n","        headers = [\"Step\",\"Actions\",\"Reward\"]\n","    # calculate returns for each reward \n","    for gamma in gammas:\n","        G = discount_rwds(rewards, gamma)\n","        data[f'G ($\\gamma$={gamma})']=G\n","        headers.append(f\"G ($\\gamma$={gamma})\")\n","   \n","        \n","    table = tabulate(data, headers=headers)\n","    print(table)\n","\n","#def state_rewards(s):\n","#    R = [-2., -2., -2., 10., 1., -1., 0.]\n","#    return R[s]\n","\n","#def state_action_reward(s,a):\n","#    R = np.zeros((n_states, n_actions))\n","#    # to do: write a matrix which specifies how much reward the agent gets for each action in each state\n","#    \n","#    # return the value of the reward the agent gets when it selection action=a while in state=s\n","#    pass \n"],"metadata":{"cellView":"form","id":"H4laQpY38Y68"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lBH02nFd7d7j"},"outputs":[],"source":["# set seed for pseudorandom number generation -- make sure our trajectories look the same\n","np.random.seed(123)\n","\n","%matplotlib inline\n","\n","from importlib import reload"]},{"cell_type":"markdown","metadata":{"id":"atctAEKA7d7l"},"source":["# Section 1: Understanding the Markov Decision Process"]},{"cell_type":"markdown","metadata":{"id":"ixl2mGBk7d7m"},"source":["## Markov Process (Markov Chain)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TmOvyMw57d7n"},"outputs":[],"source":["state_names = np.array([\"C1\", \"C2\", \"C3\", \"Pass\", \"Pub\", \"FB\", \"Sleep\"])\n","n_states = len(state_names)\n","\n","# a markov process is fully defined by the tuple <S,P>\n","S = np.arange(n_states) # numerical index for each state\n","P = np.array([[0.0,  0.5, 0.,  0.,  0.,  0.5, 0. ], # matrix representing transition probabilities between states\n","              [0.,  0.,  0.8, 0.,  0.,  0.,  0.2],\n","              [0.,  0.,  0.,  0.6, 0.4, 0.,  0. ],\n","              [0.,  0.,  0.,  0.,  0.,  0.,  1.0],\n","              [0.2, 0.4, 0.4, 0.,  0.,  0.,  0. ],\n","              [0.1, 0.,  0.,  0.,  0.,  0.9, 0. ],\n","              [0.,  0.,  0.,  0.,  0.,  0.,  1.0]])\n","\n","# print S,P\n","print(\"State Indices:\\n\",S)\n","print('==========')\n","\n","print(\"Transition Matrix:\\n\",P)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ciX0fS2I7d7o"},"outputs":[],"source":["print(\"Sum of values in each row\")\n","for ind, row in enumerate(P):\n","    print(f'P(transition from {state_names[ind]}): {sum(row):.1f}')\n","\n","print('==========')\n","\n","print(\"Sum of values in each column\")\n","for ind, col in enumerate(P.T):\n","    print(f'P(transition to {state_names[ind]}): {sum(col):.1f}')"]},{"cell_type":"markdown","metadata":{"id":"855jqheJ7d7o"},"source":["The sum of values in each row of P describe the probability of transitioning out of a specific state $s$. All transitions from state $s$ into other states will sum to 1, since when you leave state $s$ you have to end up <i>somewhere</i>.\n","\n","<b><span style=\"color: blue;\">What do the columns of P represent? </span></b>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XSkWwzlU7d7o"},"outputs":[],"source":["# collect sample trajectories\n","num_samples = 10\n","for i in range(num_samples):\n","    T = sample_MC_trajectory(S,P)  # trajectories\n","    print(f\"T{i+1} = {T}\")"]},{"cell_type":"markdown","metadata":{"id":"0bkrQ1V07d7p"},"source":["Notice how all trajectories end with 'Sleep'."]},{"cell_type":"markdown","metadata":{"id":"YrNeD_Bc7d7p"},"source":["#### Our ultimate goal in reinforcement learning is to behave in such a way that we maximize our long term future reward. \n","\n","A Markov Process has no notion of reward or of behaviour, so let's add these in one at a time. "]},{"cell_type":"markdown","metadata":{"id":"XbGAJdsD7d7p"},"source":["## Markov Reward Proccess (MRP)"]},{"cell_type":"markdown","metadata":{"id":"OEd2_BKe7d7q"},"source":["\n","### Note: rewards can also be action-dependent! \n","Notice above we have $R^{a}_{s}$ -- upon leaving a state, the environment may reward the agent differently depending on the action taken. \n","\n","For simplicity in this tutorial we will just take the rewards to be the same for all actions taken. \n","\n","<b><span style='color:DarkGreen'> Homework: write a reward function as a matrix of size (n_states x n_actions), which takes current state and action as arguments and returns the appropriate reward value (you pick what those rewards are). Hint: start with a reward matrix in which only one action leads to rewards.</span></b>\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qwHxu3TY7d7q"},"outputs":[],"source":["###########\n","# Homework \n","###########\n","## recall  R = [-2., -2., -2., 10., 1., -1., 0.]\n","\n","## To Do: write the reward function as a matrix\n","\n","## Bonus: write the reward function as an actual python function that takes either state or state/action \n","#   as arguments and returns the reward: \n","\n","#def state_rewards(s):\n","#    R = [-2., -2., -2., 10., 1., -1., 0.]\n","#    return R[s]\n","\n","#def state_action_reward(s,a):\n","#    R = np.zeros((n_states, n_actions))\n","#    # to do: write a matrix which specifies how much reward the agent gets for each action in each state\n","#    \n","#    # return the value of the reward the agent gets when it selection action=a while in state=s\n","#    pass    "]},{"cell_type":"markdown","metadata":{"id":"tGDvqblR7d7q"},"source":["### Return\n","How do we think about long term rewards? Are rewards in the future as valuable as rewards now? The parameter $\\gamma$ describes how much we discount rewards that are further away from our current state. You can think of $\\gamma$ as <b> the present value of future rewards</b>. \n","\n","\n","<b>DEF</b> The return (also called gain) $G_{t}$ is the total discounted reward from timestep $t$: \n","> $G_t  = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + ... $\n",">\n","> $\\quad \\ = \\sum_{k=0}^{\\infty} \\gamma^{k} R_{t+k+1}$\n","\n","$\\gamma>0$ values immediate reward over delayed reward.\n","\n","If $\\gamma = 0$ this means we are maximlly nearsighted -- we care only about the immediate reward\n","\n","If $\\gamma = 1$ this means we are maximally farsighted -- we care about all rewards equally, possibly to an infinite horizon (if the MRP does not terminate) \n","\n","Note: $G_t$ is the return in whatever state reached at timestep t in a single trajectory."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q_TiwkNP7d7q"},"outputs":[],"source":["# collect sample trajectories and compute return \n","# S & P already defined above for Markov Chain\n","R = np.array([-2., -2., -2., 10., 1., -1., 0.])\n","gamma = 0.9\n","\n","# collect sample trajectories (states visited) and rewards\n","T, rewards = sample_MRP_trajectory(S,P,R)\n","print(f\"trajectory   = {T}\\nrewards = {rewards}\")\n","\n","# compute returns \n","G = discount_rwds(rewards, gamma)\n","print(f\"gamma = {gamma} \\nreturn = {G}\")\n","\n","# show how return is calculated for t=0\n","first_element_return(T,rewards,gamma)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iDehOWjL7d7q"},"outputs":[],"source":["show_trajectory_table(T, rewards, gammas=[gamma])"]},{"cell_type":"markdown","metadata":{"id":"4VYqCv5H7d7q"},"source":["Ask yourself: how was the $G$ for each step calculated?"]},{"cell_type":"markdown","metadata":{"id":"wOXfGVIK7d7q"},"source":["Notice how the value of $\\gamma$ impacts the way we value each state visited. \n","\n","<b><span  style=\"color: blue;\"> Using the same trajectory generated in the cell above, let's compute return using a few different values for $\\gamma$. </span></b>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4YesnEDP7d7q"},"outputs":[],"source":["# try for different values of gamma:\n","gammas = [0.1, 0.2, 0.5, 0.9]\n","\n","show_trajectory_table(T,rewards,gammas)"]},{"cell_type":"markdown","metadata":{"id":"ZuIhOwRl7d7q"},"source":["Tip: you can click on the above 3 code cells a couple times to generate different example trajectories!"]},{"cell_type":"markdown","metadata":{"id":"_YcQw0Kn7d7q"},"source":["### What is the relationship between reward, return, and value? \n","Rewards are the feedback from individual states of the envrionment. \n","\n","Return is the total discounted reward from timestep t into the future. \n","\n","<b>DEF</b> We define value, $v(s)$ to be is the expected return starting in state s:\n","> $v(s) = \\mathbb{E}[G_{t} | S_{t} = s]$"]},{"cell_type":"markdown","metadata":{"id":"5vj7MMkx7d7r"},"source":["Consider a simple example where $\\gamma = 0$. No matter what trajectory you take from each state, the value is just equal to the immediate reward (since all future timesteps are discounted to 0). State values are: \n","<img src=\"attachment:student_reward_gamma0.png\" width=\"300\"/>\n","\n","What about when $\\gamma = 0.9$? In this case, state values are: \n","<img src=\"attachment:student_reward_gamma09.png\" width=\"300\"/>\n","\n","For now, let's take for granted that these are the values. (To test for yourself, we can *sample trajectories* starting in each state and compute the *average return* over all these trajectories. The more samples you take, the more accurate this estimate of the value will be. This is called <b>Monte Carlo</b> sampling and we will discuss this in more detail later.)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zvLHf2GW7d7r"},"outputs":[],"source":["get_MRP_values(S,P,R,gamma=0.9,num_runs=500)"]},{"cell_type":"markdown","metadata":{"id":"X3BNYKEe7d7r"},"source":["<b><span style='color:blue'> Holding $\\gamma$ fixed, how does the estimate of the value change if you change the number of runs? For 10 runs? For 1000 runs? </span></b>"]},{"cell_type":"markdown","metadata":{"id":"EeoPPfkZ7d7r"},"source":["## Markov Decision Process\n","\n","In the previous example, we only considered moving between states (effectively, there was only one action to choose, which was \"leave this state\"). \n","\n","Now we will consider what happens when we have some options for how to behave, and those options have different consequences -- i.e. the probability of transitioning between states might be different depending on the actions we choose. \n","\n","Consider the following two-action system: \n","   - in <span style='color:red'>red</span> is the action we considered previously, let's call this \"chill\"\n","   - in <span style='color:blue'>blue</span> we have \"study\" which has different probabilities of transitioning between states\n","\n","<div><img src=\"attachment:2_actions.png\" width=\"700\"/></div>\n"]},{"cell_type":"markdown","metadata":{"id":"9yc4vOUW7d7r"},"source":["## Markov Decision Proccess (MDP)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZshhxVXA7d7r"},"outputs":[],"source":["# define the state transition function as a matrix for each action\n","P_chill = np.array([[0.0,  0.5, 0.,  0.,  0.,  0.5, 0. ], # matrix representing transition probabilities between states\n","                  [0.,  0.,  0.8, 0.,  0.,  0.,  0.2],\n","                  [0.,  0.,  0.,  0.6, 0.4, 0.,  0. ],\n","                  [0.,  0.,  0.,  0.,  0.,  0.,  1.0],\n","                  [0.2, 0.4, 0.4, 0.,  0.,  0.,  0. ],\n","                  [0.1, 0.,  0.,  0.,  0.,  0.9, 0. ],\n","                  [0.,  0.,  0.,  0.,  0.,  0.,  1.0]])\n","\n","P_study = np.array([[0.0,  1., 0.,  0.,  0.,  0., 0. ], # matrix representing transition probabilities between states\n","                  [0.,  0.,  1., 0.,  0.,  0.,  0.],\n","                  [0.,  0.,  0.,  1., 0.,  0.,  0.],\n","                  [0.,  0.,  0.,  0., 0.5, 0., 0.5],\n","                  [0., 0., 0., 0.,  0.7,  0.,  0.3 ],\n","                  [1.0, 0.,  0.,  0.,  0.,  0., 0. ],\n","                  [0.,  0.,  0.,  0.,  0.,  0.,  1.0]])\n","\n","P = np.array([P_chill,P_study])\n","\n","# the state transition function is a tensor of size (# actions, # of states, #of states)\n","print('Transition Tensor Dimensions:\\n',P.shape)"]},{"cell_type":"markdown","metadata":{"id":"DL_HQMp07d7r"},"source":["For the transition function, we have a tensor of size: \n","(number of actions, number of states, number of states). The first dimension represents the actions we could take, the second dimension represents the possible states we could be in currently, and the third dimension represents the states we could move into. Note that this ordering is not fixed. We can slice the tensor in any way that makes sense. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3y1TlkZf7d7r"},"outputs":[],"source":["# recall: chill=0  /  study=1 \n","action_sequence = [0, 1, 1, 0, 0, 1, 1, 1]\n","starting_state = 'C1'\n","\n","T, actions, rewards = sample_MDP_trajectory(S, P, R, starting_state, action_sequence)\n","\n","show_trajectory_table(T, rewards, gammas=[0.9], actions=actions)"]},{"cell_type":"markdown","metadata":{"id":"fT2gm7HL7d7r"},"source":["<b><span style='color:blue'>What is going on in the table above for the actions after the Sleep step? </span></b>"]},{"cell_type":"markdown","metadata":{"id":"P-L8I8cF7d7r"},"source":["For the most part, we don't know the full sequence of actions from the start. Instead, we can make choices at each step. The rule for how we select actions is called the <b>policy</b>, and is denoted $\\pi(a|s)$. \n","\n","<b> DEF </b> A policy  $\\pi(a|s)$ is a probability distribution over actions given the current state of the environment, i.e. \n","> $\\pi(a|s)$ = P($A_{t}$ = a | $S_{t}$ = s)\n","\n","One policy is a random policy -- just flip a coin at every step -- will you sleep or study? "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3YpbuL8O7d7r","executionInfo":{"status":"ok","timestamp":1681898640062,"user_tz":-120,"elapsed":2,"user":{"displayName":"Dongyan Lin","userId":"09293928753917669945"}},"outputId":"efa15266-04a2-45cb-9331-5c87b70f92e0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Step    Actions      Reward    G ($\\gamma$=0.1)    G ($\\gamma$=0.9)\n","------  ---------  --------  ------------------  ------------------\n","C1      Chill            -2               -2.21                1.87\n","C2      Chill            -2               -2.1                 4.3\n","C3      Chill            -2               -1                   7\n","Pass    Chill            10               10                  10\n","Sleep                     0                0                   0\n"]}],"source":["T, actions, rewards = sample_MDP_trajectory(S, P, R, starting_state, action_sequence=None)\n","\n","show_trajectory_table(T, rewards, gammas=[0.1, 0.9], actions=actions)"]},{"cell_type":"markdown","metadata":{"id":"5jjkrD1D7d7r"},"source":["Now that the reward $R$ can depend on the action $a$, so can the value function (since it is by definition the expected return, or expcted sum of rewards):\n","\n","<b>DEF</b> We define state-action-value, $q(s,a)$ to be is the expected return starting with taking action a in state s, and following the policy $\\pi$ thereafter:\n",">\n","> $q_\\pi(s,a) = \\mathbb{E}_\\pi[G_{t} | S_{t} = s, A_{t} = a]$\n",">\n"]},{"cell_type":"markdown","metadata":{"id":"xdy67WcN7d7r"},"source":["<b>TASK</b> Fill in Section 1 (Basic Concepts) and 2 (Markov Decision Process) of the worksheet"]},{"cell_type":"markdown","metadata":{"id":"unEW5Iqm7d7r"},"source":["# Section 2: Understanding the Environment"]},{"cell_type":"markdown","metadata":{"id":"W_1g5EuH7d7r"},"source":["## Starting with a simple environment: Trackworld -- a linear gridworld\n","\n","We start with the simplest gridworld -- a single row with a few columns. \n","<b><span style=\"color: blue;\">Here you will write some code to determine the state transitions probabilities in P. </span></b>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ffWK43FK7d7s"},"outputs":[],"source":["class TrackWorld(object):\n","    def __init__(self, n_states=10): \n","        self.n_states   = n_states\n","        self.all_states = np.arange(self.n_states)\n","        \n","        self.actions = {'Left':0, 'Right':1}\n","        self.n_acts  = len(self.actions)\n","        \n","        self.start    = 0\n","        self.terminal = self.n_states-1\n","        self.done     = False\n","        \n","        self.P       = self.transition_function()\n","    \n","        self.R       = self.reward_function() \n","        \n","    def transition_function(self):\n","        P = np.zeros((self.n_acts,self.n_states, self.n_states))\n","        # add transitions between neighbouring states \n","        \n","        ## TO DO: SET THE TRANSITIONS FOR MOVES TO THE LEFT\n","        for i_state in range(1, self.n_states):\n","            ... ## FILL THIS IN\n","            \n","        ## TO DO: SET THE TRANSITIONS FOR MOVES TO THE RIGHT \n","        for i_state in range(0,self.n_states-1):\n","            ... ## FILL THIS IN\n","\n","        \n","        # what happens in the end states? \n","        P[0,0,0] = 1\n","        P[1,self.terminal,self.terminal]=1\n","\n","        return P\n","    \n","    def reward_function(self):\n","        R = np.zeros(self.n_states)\n","        R[self.terminal] = 10\n","        return R \n","        \n","    def step(self,state,action):\n","        if action not in self.actions.keys():  # check if move is valid\n","            raise Exception('Agent has tried an invalid action!')\n","        else:\n","            transition_probs = self.P[self.actions[action],state, :]\n","            next_state = np.random.choice(self.all_states, p=transition_probs)\n","            reward = self.R[state]\n","            if state == self.terminal:\n","                self.done = True\n","        return next_state, reward, self.done"]},{"cell_type":"markdown","metadata":{"id":"BwE2HCLb7d7s"},"source":["<b><span style='color:blue'>Make a new instance of the trackworld class, and make a move in the environment using the function called 'step'.</span></b>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GR11XILZ7d7s","outputId":"f5e89528-d960-4c75-a285-94f8196a8aab"},"outputs":[{"name":"stdout","output_type":"stream","text":["State 5 / Action Right => State 6 / Reward 0.0\n"]}],"source":["# make an instance of trackworld\n","tw = TrackWorld()\n","\n","# test a state an action and make sure it does what it's supposed to do \n","state = 5\n","action= 'Right'\n","next_state, reward, is_done = tw.step(state,action)\n","print(f\"State {state} / Action {action} => State {next_state} / Reward {reward}\")"]},{"cell_type":"markdown","metadata":{"id":"bVK435R87d7s"},"source":["#### Homework: Write the transition function for a windy trackworld, with wind blowing to the left. </span></b>\n","    \n","<span  style=\"color: DarkGreen;\">This means that when the agent moves to the left, the wind pushes it such that it has a probability $k \\in [0,1]$ of moving two steps to the left instead of one (and probability $1-k$ of moving the usual 1 step to the left). When the agent is moving to the right, it is moving against the wind so it has a probability of $k$ of staying in the same space and a probability of $1-k$ of moving one step to the right. You can think of $k$ as the strength of the wind. You can pass $k$ as a parameter to your WindyTrack instance. Some starter code is given below. </span>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wkJekJWH7d7s"},"outputs":[],"source":["###########\n","# Homework \n","###########\n","class WindyTrack(TrackWorld):\n","    def __init__(self,n_states=10, wind_strength=0.3):\n","        super().__init__(n_states=n_states)\n","        self.k = wind_strength\n","    \n","    def transition_function(self):\n","        # compute probabilities of transitions using self.k here \n","        pass"]},{"cell_type":"markdown","metadata":{"id":"z3IhnlmS7d7s"},"source":["## Adding another dimension to Trackworld: Gridworld\n","\n","Here we have added multiple rows. For simplicity let's do 5 rows and 5 columns = 25 states total. This is enough to do interesting things, but not so many that it's hard to keep track of what's going on with state transitions. \n","\n","This also adds two more possible moves: \"Up\" and \"Down\". \n","\n","In each episode (or \"run\"), the agent starts at a random state on the map, and ends at the terminal state located on the bottom right corner. To make sure the agent moves to the terminal state in as few steps as possible, we enforce a small punishment (i.e. negative reward), $r=-0.01$, on every step the agent takes that does not lead to the terminal state.\n","\n","Optionally, we can block out a few states that the agent cannot move to. More on that later.\n","\n","We will use the following gridworld as an example, and write some code to turn this schematic into an environment we can interact with. \n","<div><img src=\"attachment:gridworld_.png\" width=\"500\"/></div>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iK40NeBq7d7s"},"outputs":[],"source":["n_rows, n_cols = 5, 5\n","step_penalization = -0.01\n","gw_open = GridWorld(rows=n_rows,cols=n_cols,\n","                  #obstacles=[(2,2),(2,3),(2,4),(3,2)], \n","                  rewards={(4,4):10},  # Note that python indexing starts with 0\n","                  terminals=[(4,4)],\n","                  step_penalization=step_penalization,\n","                  actionlist=['Down','Up','Right','Left'],\n","                  view_labels=True\n","                 )\n","\n","print('Rewards Function:\\n', gw_open.R)\n","\n","print(f'Transition Matrix for State {gw_open.state}:\\n', gw_open.P[:,gw_open.state,:].T)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1Io9Q1gA7d7s"},"outputs":[],"source":["# Visualizing the reward function another way\n","def plot_reward_map(env):\n","    R_map = plt.imshow(env.R.reshape(n_rows,n_cols))\n","    plt.colorbar(R_map)\n","    plt.show()\n","    \n","plot_reward_map(gw_open)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I0cvGQ727d7s"},"outputs":[],"source":["## take a single step in the environment\n","original_state = gw_open.state\n","action_name = 'Down'\n","action_index = gw_open.action_list.index(action_name)\n","updated_state, reward, done, info = gw_open.step(action_index)\n","print(f\"From state {original_state}, Agent chose action {action_name}, received reward {reward}, and transitioned to state {updated_state}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GJVOtMY97d7s"},"outputs":[],"source":["class RandomAgent(object):\n","    def __init__(self, num_actions):\n","        self.num_actions = num_actions\n","    def select_action(self, state): ## THIS IS THE POLICY FOR BEHAVIOUR\n","        # state actually does nothing in this agent, but we will want our action selection\n","        # in later agents to be based on the state we're in so we are setting up the pattern to follow\n","        action = np.random.choice(self.num_actions)\n","        return action"]},{"cell_type":"markdown","metadata":{"id":"-blt15Sp7d7s"},"source":["<b><span style='color:blue'>Here we will write the code that takes steps in the environment. Make a call to the random agent to get an action, and then take a step in the environment. </span></b>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iuAKjijS7d7s"},"outputs":[],"source":["# Write a function to count the number of steps it takes for the agent to reach the end \n","def navigate(gw, agent, num_runs=250):\n","    track_steps_to_finish = []\n","    track_rewards_achieved= []\n","    for run in range(num_runs):\n","        done = False\n","        move_counter   = 0\n","        reward_counter = 0\n","        # take moves in the environment until reach the terminal state:\n","        while not done:\n","            current_state = gw.state\n","            \n","            ## TO DO -- get an action from the random agent\n","            action = ...  ## FILL THIS IN \n","            \n","            ## TO DO -- take a step in the environment. \n","            ##        from the cell above, what information does the step function return?\n","            updated_state, reward, done, info = ...  ## FILL THIS IN\n","            \n","            # keep running tally of rewards achieved\n","            reward_counter += reward \n","            \n","            # keep track of how many steps it takes to get to the end\n","            move_counter += 1 \n","            \n","            #if run == 0:\n","            #    print(f'{current_state}/{action}-->{updated_state}')\n","\n","        track_steps_to_finish.append(move_counter)\n","        track_rewards_achieved.append(reward_counter)\n","        gw.reset()\n","    return track_steps_to_finish, track_rewards_achieved"]},{"cell_type":"markdown","metadata":{"id":"a9qWpvA37d7s"},"source":["# Use a Random Agent to Move Through the Gridworld"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2vBkZC3W7d7s"},"outputs":[],"source":["random_walker = RandomAgent(len(gw_open.action_list)) \n","\n","open_gw_steps,open_gw_rwds = navigate(gw_open, random_walker)\n","\n","\n","# plot performance\n","fig, ax = plt.subplots(2,1, sharex=True)\n","ax[0].plot(running_mean(open_gw_steps, N=10))\n","ax[1].plot(running_mean(open_gw_rwds,N=10))\n","ax[1].set_ylim(-1,10.1)\n","ax[0].set_ylabel('Steps')\n","ax[1].set_ylabel('Rewards')\n","ax[1].set_xlabel('Episode')\n","ax[0].set_title('Performance for Random Agent in Openfield Environment')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hpU8yc_O7d7t"},"outputs":[],"source":["gw_obst = GridWorld(rows=n_rows,cols=n_cols,\n","                  obstacles=[(2,2),(2,3),(2,4),(3,2)], \n","                  rewards={(4,4):10},\n","                  terminals=[(4,4)],\n","                  step_penalization=step_penalization,\n","                  actionlist=['Down','Up','Right','Left'],\n","                  view_labels=True\n","                 )\n","\n","obst_gw_steps,obst_gw_rwds = navigate(gw_obst, random_walker)\n","\n","## Plot performance \n","fig, ax = plt.subplots(2,1, sharex=True)\n","ax[0].plot(running_mean(obst_gw_steps, N=10))\n","ax[1].plot(running_mean(obst_gw_rwds,N=10))\n","ax[1].set_ylim(-1,10.1)\n","ax[0].set_title('Performance for Random Agent in Environment with Obstacles')\n","ax[0].set_ylabel('Steps')\n","ax[1].set_ylabel('Rewards')\n","ax[1].set_xlabel('Episode')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HJBD0C3L7d7t"},"outputs":[],"source":["gw_cliff = GridWorld(rows=n_rows,cols=n_cols,\n","                  rewards={(4,4):10},\n","                  terminals=[(n_rows-1,c) for c in range(n_cols)],\n","                  step_penalization=step_penalization,\n","                  actionlist=['Down','Up','Right','Left'],\n","                  view_labels=True\n","                 )\n","\n","print(gw_cliff.terminal)\n","\n","cliff_gw_steps, cliff_gw_rwds = navigate(gw_cliff, random_walker)\n","\n","## Plot performance \n","fig, ax = plt.subplots(2,1, sharex=True)\n","ax[0].plot(running_mean(cliff_gw_steps, N=10))\n","ax[1].plot(running_mean(cliff_gw_rwds,N=10))\n","ax[1].set_ylim(-1,10.1)\n","ax[0].set_title('Performance for Random Agent in Environment with Obstacles')\n","ax[0].set_ylabel('Steps')\n","ax[1].set_ylabel('Rewards')\n","ax[1].set_xlabel('Episode')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DvMCxnTq7d7t"},"outputs":[],"source":["## Plot performance \n","smoothing = 10\n","\n","fig, ax = plt.subplots(2,1, sharex=True)\n","ax[0].plot(running_mean(open_gw_steps, N=smoothing),label='open')\n","ax[0].plot(running_mean(obst_gw_steps, N=smoothing),label='obstacles')\n","ax[0].plot(running_mean(cliff_gw_steps, N=smoothing),label='cliff')\n","\n","ax[0].set_title('Performance for Random Agent in All Environments')\n","ax[0].set_ylabel('Steps')\n","ax[0].legend(bbox_to_anchor=(1.01,0.95))\n","\n","ax[1].plot(running_mean(open_gw_rwds,N=smoothing),label='open')\n","ax[1].plot(running_mean(obst_gw_rwds,N=smoothing),label='obstacles')\n","ax[1].plot(running_mean(cliff_gw_rwds,N=smoothing),label='cliff')\n","ax[1].set_ylim(-1,10.1)\n","ax[1].set_ylabel('Rewards')\n","ax[1].set_xlabel('Episode')\n","\n","#avg_steps = [np.mean(open_gw_steps),np.mean(obst_gw_steps),np.mean(cliff_gw_steps)]\n","#avg_rwds  = [np.mean(open_gw_rwds),np.mean(obst_gw_rwds),np.mean(cliff_gw_rwds)]\n"]},{"cell_type":"markdown","metadata":{"id":"wOhOb3gQ7d7t"},"source":["What do you observe from the performance in the obstacles and cliff gridworlds?"]},{"cell_type":"markdown","metadata":{"id":"SyGK81Rc7d7t"},"source":["<b><span style=\"color: blue;\"> Try changing the step penalization above and rerun these simulations -- see what happens to the reward collected with the random agent in each type of environment. </span></b>"]},{"cell_type":"markdown","metadata":{"id":"jqJW8YA17d7t"},"source":["# Some larger example environments"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rRfmW_qI7d7t"},"outputs":[],"source":["gw_open_large = GridWorld()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_BVOukcH7d7t"},"outputs":[],"source":["gw_random_obs = GridWorld_random_obstacle()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7nSutPI47d7t"},"outputs":[],"source":["gw_divided = GridWorld_bar()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p7teOzfd7d7t"},"outputs":[],"source":["gw_4rooms = GridWorld_4rooms()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yTq4xoZX7d7t"},"outputs":[],"source":["gw_tunnel = GridWorld_tunnel()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g4QljCYB7d7t"},"outputs":[],"source":["gw_hairpinmaze = GridWorld_hairpin()"]},{"cell_type":"markdown","metadata":{"id":"SfiN-IcF7d7t"},"source":["<b><span style='color:DarkGreen'>Homework: Test the random walk agent on the `gridworld_open` and `gridworld_open_large` environments, and plot the rewards achieved in both.</span></b>"]},{"cell_type":"markdown","metadata":{"id":"eatmCY607d7t"},"source":["These larger spaces are unlikely to yield much reward, especially for agents using a random policy. \n","\n","Next, we will look at some ways agents learn how to behave by evaluating how much reward each action is likely to lead to in the long term."]},{"cell_type":"markdown","metadata":{"id":"LeLVAG0_7d7t"},"source":["# Section 3: Understanding Agents & *Actually Learning* from Trial and Error"]},{"cell_type":"markdown","metadata":{"id":"c6ZCE9ZW7d7t"},"source":["### First we will choose an environment to test all of our learners in "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yKL9KioI7d7t"},"outputs":[],"source":["n_rows, n_cols = 10,10\n","step_penalization = -0.01\n","env = GridWorld(rows=n_rows,cols=n_cols,\n","                  #obstacles=[(3,2),(3,3),(3,4),(4,2),(5,2),(3,5),(3,6),(3,7)], \n","                  rewards={(5,5):10},\n","                  terminals=[(5,5)],\n","                  step_penalization=step_penalization,\n","                  actionlist=['Down','Up','Right','Left'],\n","                  view_labels=False)"]},{"cell_type":"markdown","metadata":{"id":"9eHky_La7d7t"},"source":["### Let's set up the basic framework that all of our tabular agents will use\n","\n","<b><span style='color:blue'> Write the $\\epsilon$-greedy action selection function. </span></b>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VLx9tYh77d7u"},"outputs":[],"source":["class TabularAgent(object):\n","    def __init__(self, nstates, nactions, \n","                 learning_rate=0.1, discount=0.9, epsilon=1.0):\n","        \n","        self.num_actions = nactions\n","        self.action_space = np.arange(self.num_actions)\n","\n","        # this agent selects actions from a table of <state,action> values which we initalize randomly\n","        #self.q_table = np.random.uniform(low=-1, high=1, size=(nstates, nactions))\n","        self.q_table = np.zeros((env.nstates, env.nactions))\n","\n","        # parameters for learning\n","        self.epsilon       = epsilon\n","        self.learning_rate = learning_rate\n","        self.discount      = discount\n","        \n","    def choose_action(self, state):\n","        # this agent uses epsilon-greedy action selection, meaning that it selects \n","        # the greedy (highest value) action most of the time, but with epsilon probability\n","        # it will select a random action -- this helps encourage the agent to explore\n","        # unseen trajectories\n","        \n","        ## TO DO -- write action selection for an epsilon-greedy policy \n","        if np.random.random()>self.epsilon:\n","            # take the action which corresponds to the highest value in the q table at that row (state)\n","            ...  ## FILL THIS IN\n","        else: \n","            ...  ## FILL THIS IN\n","        return action"]},{"cell_type":"markdown","metadata":{"id":"72x2ZlrZ7d7u"},"source":["Now that we've told the agent how to select actions based on the values, the next question is: how do we know the values of each state-action pair, i.e. $q(s,a)$?"]},{"cell_type":"markdown","metadata":{"id":"E5bgC-Ji7d7u"},"source":["## Monte Carlo Agents Update Q Values From Computed Return\n","\n","<b><span style='color:blue'> Write Q value update using Monte Carlo learning. </span></b>"]},{"cell_type":"markdown","metadata":{"id":"LqsqheAf7d7u"},"source":["Recall: Monte Carlo sampling is where you sample trajectories and calculate their return $G$ to estimate state-action-values. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0Ft4m90h7d7u"},"outputs":[],"source":["# MC Learning of the state value function\n","class MC_Agent(TabularAgent):\n","    def __init__(self, nstates,nactions, learning_rate=0.1, discount=0.95, epsilon=1.0):\n","        super().__init__(nstates,nactions, learning_rate=learning_rate , discount=discount, epsilon=epsilon)\n","\n","    def update_q_table(self, current_state, current_action, computed_return):\n","        # this function describes how the Q table gets updated so the agent can make \n","        # better choices based on what it has experienced from the environment \n","        current_q = self.q_table[current_state, current_action]\n","        \n","        ## TO DO -- write the Q value update step\n","        new_q = ...  ## FILL THIS IN\n","        self.q_table[current_state, current_action] = new_q\n","        \n","    def navigate(self, env, num_episodes, random_start=False, start=0):\n","        # set how we will decay the randomness of action selection over the course of training\n","        start_eps_decay = 1\n","        end_eps_decay = num_episodes//2\n","        epsilon_decay_value = self.epsilon/(end_eps_decay-start_eps_decay)\n","\n","        # initialize empty list for keeping track of rewards achieved per episode\n","        reward_tracking=[]\n","        max_steps= 1000\n","\n","        for episode in range(num_episodes):\n","            env.reset()\n","            # initalize reward counter\n","            total_reward=0\n","\n","            # get first state and action\n","            if random_start:\n","                state=np.random.choice(env.nstates)\n","            else:\n","                state=start\n","            \n","            states, actions, rewards = [], [], []\n","            for step in range(max_steps):\n","                action = self.choose_action(state)\n","                # take a step in the environment\n","                next_state, reward, done, _ = env.step(action)\n","\n","                total_reward+=reward\n","\n","                if not done:\n","                    states.append(state)\n","                    actions.append(action)\n","                    rewards.append(reward)\n","                else:\n","                    break\n","                state=next_state\n","            \n","            # compute return\n","            returns = discount_rwds(rewards, gamma=0.9)\n","            # use the returns from the sampled trajectory to update the value of every s-a pair experienced\n","            for i in range(len(states)):\n","                self.update_q_table(states[i],actions[i],returns[i])\n","            \n","            reward_tracking.append(total_reward)\n","\n","            if end_eps_decay >= episode >= start_eps_decay:\n","                self.epsilon -= epsilon_decay_value\n","\n","        return reward_tracking"]},{"cell_type":"markdown","metadata":{"id":"wiDlj87T7d7u"},"source":["### How does the MC Agent Perform in this environment? "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2E5Myfdn7d7u"},"outputs":[],"source":["## let's look at the reward function on the environment:\n","plot_reward_map(env)\n","\n","## run the MC agent through the environment to learn the reward landscape \n","mc_learner = MC_Agent(env.nstates, env.nactions)\n","mc_rewards = mc_learner.navigate(env, num_episodes=5000)\n","\n","## show the rewards achieved \n","plt.plot(mc_rewards)\n","plt.xlabel('Episode')\n","plt.ylabel('Reward Obtained Per Episode')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"nXRMNWi97d7u"},"source":["### Let's look at what the agent has learned by inspecting the Q values in its table"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ogk5Sw-i7d7u"},"outputs":[],"source":["## show the value map learned by the agent\n","# Show values for each action\n","fig, ax = plt.subplots(1,4)\n","for i in range(4):\n","    ax[i].imshow(mc_learner.q_table[:,i].reshape(n_rows,n_cols))\n","    ax[i].set_title(env.action_list[i])\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bew22iSC7d7u"},"outputs":[],"source":["# show average across actions ~ V(s)\n","a = np.mean(mc_learner.q_table, axis=1).reshape(n_rows,n_cols)\n","\n","f = plt.imshow(a)\n","plt.colorbar(f)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"J1rfPOEu7d7u"},"source":["## TD Learning Methods: SARSA and Q-Learning\n","\n","Now we will look at an alternative strategy for learning state-action-values: Temporal difference (TD) learning. TD learning uses one piece of experience information (one-step observed reward) plus its guess of future value to update its guess of the state value. \n","\n","Q learning and SARSA are two different methods of doing TD learning. They differ based on which policy they use for learning. SARSA is an **on** policy algorithm, meaning it uses the same policy for behaviour (selecting actions) and for optimization (learning updates). Q-learning is an **off** policy algorithm, meaning it uses a different policy for behaving ($\\epsilon$-greedy policy) and for optimizing (pure greedy policy). "]},{"cell_type":"markdown","metadata":{"id":"vhnH3En37d7u"},"source":["<b><span style='color:blue'>Write the Q value update using SARSA-learning. Also, re-write the navigation function to maintain the relevant information needed to do SARSA</span></b>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9gwwgTHO7d7u"},"outputs":[],"source":["## SARSA -- same same but different\n","class SARSA_Agent(TabularAgent):\n","    def __init__(self, nstates,nactions, learning_rate=0.1, discount=0.95, epsilon=1.0):\n","        super().__init__(nstates,nactions, learning_rate=learning_rate , discount=discount, epsilon=epsilon)\n","\n","    #updates q values after each step \n","    # similar to Q update but with one key difference -- we use the action we *actually* took\n","    # rather than guessing we took the max value action\n","    # with epsilon probability, we will have actually taken a random action, so SARSA wants to account for that \n","    \n","    ## TO DO -- write the update to the Q table using SARSA. What arguments are needed for this function?\n","    def update_q_table(self,current_state,current_action,reward,next_state,next_action):\n","        current_q = self.q_table[current_state, current_action]\n","        future_q = ...  # FILL THIS IN\n","        \n","        ## TO DO -- write the Q Value update using SARSA\n","        new_q = ... # FILL THIS IN\n","        self.q_table[current_state, current_action] = new_q\n","        \n","    def navigate(self, env, num_episodes, random_start=False, start=0): # takes sarsa_agent as input \n","        #-- will not work w Q_agent bc takes additional argument of next_state in update_q_table function\n","        # set how we will decay the randomness of action selection over the course of training\n","        start_eps_decay = 1\n","        end_eps_decay = num_episodes//2\n","        epsilon_decay_value = self.epsilon/(end_eps_decay-start_eps_decay)\n","\n","        # initialize empty list for keeping track of rewards achieved per episode\n","        reward_tracking=[]\n","        max_steps= 100\n","\n","        for episode in range(num_episodes):\n","            env.reset()\n","            # initalize reward counter\n","            total_reward=0\n","\n","            # get first state and action\n","            if random_start:\n","                state=np.random.choice(env.nstates)\n","            else:\n","                state=start\n","            action = self.choose_action(state)\n","            \n","            ## TO DO -- write the procedure for a single step for SARSA \n","            ## Hint: Notice how next_action is used in self.update_q_table and env.step?\n","            ## That's because the target policy and the behavioural policy are the same in SARSA.\n","            \n","            for step in range(max_steps):\n","                # take a step in the environment\n","                next_state, reward, done, _ = env.step(action)\n","\n","                total_reward+=reward\n","\n","                if not done:\n","                    next_action = ... ## FILL THIS IN\n","                    self.update_q_table(state,action,reward,next_state,next_action)\n","                else:\n","                    self.q_table[state,action]= 0 \n","                    break\n","                state=next_state\n","                action=next_action\n","\n","            reward_tracking.append(total_reward)\n","\n","            if end_eps_decay >= episode >= start_eps_decay:\n","                self.epsilon -= epsilon_decay_value\n","\n","        return reward_tracking"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mSg66qp67d7u"},"outputs":[],"source":["sarsa_learner = SARSA_Agent(env.nstates, env.nactions)\n","sarsa_rewards = sarsa_learner.navigate(env, num_episodes=5000)\n","\n","plt.plot(running_mean(sarsa_rewards,N=30))\n","plt.xlabel('Episode')\n","plt.ylabel('Reward Obtained Per Episode')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vli4UufJ7d7u"},"outputs":[],"source":["# show values under different learners\n","# Show values for each action\n","fig, ax = plt.subplots(1,4)\n","for i in range(4):\n","    ax[i].imshow(sarsa_learner.q_table[:,i].reshape(n_rows,n_cols))\n","    ax[i].set_title(env.action_list[i])\n","plt.show()\n","\n","# show average across actions ~ V(s)\n","a = np.mean(sarsa_learner.q_table, axis=1).reshape(n_rows,n_cols)\n","\n","f = plt.imshow(a)\n","plt.colorbar(f)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"z91cggho7d7v"},"source":["### Because Q and SARSA need to keep track of slightly different information, we will write two functions for how they navigate the environment \n","\n","<b><span style='color:blue'>Write the Q value update using Q-learning. </span></b>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FIYoKU9c7d7v"},"outputs":[],"source":["# TD Agents\n","class Q_Agent(TabularAgent):\n","    def __init__(self, nstates,nactions, learning_rate=0.1, discount=0.95, epsilon=1.0):\n","        super().__init__(nstates,nactions, learning_rate=learning_rate , discount=discount, epsilon=epsilon)\n","\n","    def update_q_table(self, current_state, current_action, reward, new_state):\n","        # this function describes how the Q table gets updated so the agent can make \n","        # better choices based on what it has experienced from the environment \n","        current_q = self.q_table[current_state, current_action]\n","        max_future_q = ... # FILL THIS IN\n","        \n","        ## TO DO -- write the Q value update using Q-learning\n","        new_q = ...  ## FILL THIS IN\n","        self.q_table[current_state, current_action] = new_q\n","        \n","    def navigate(self, env, num_episodes, random_start=False, start=0):\n","        # set how we will decay the randomness of action selection over the course of training\n","        start_eps_decay = 1\n","        end_eps_decay = num_episodes//2\n","        epsilon_decay_value = self.epsilon/(end_eps_decay-start_eps_decay)\n","\n","        # initialize empty list for keeping track of rewards achieved per episode\n","        reward_tracking=[]\n","        max_steps= 1000\n","\n","        for episode in range(num_episodes):\n","            env.reset()\n","            # initalize reward counter\n","            total_reward=0\n","\n","            # get first state and action\n","            if random_start:\n","                state=np.random.choice(env.nstates)\n","            else:\n","                state=start\n","            \n","            ## TO DO -- write the procedure for a single step for Q-Learning\n","            ## Hint: The behavioural policy and the target policy are different in Q-learning.\n","            \n","            for step in range(max_steps):\n","                action = ... ## FILL THIS IN\n","                # take a step in the environment\n","                next_state, reward, done, _ = env.step(action)\n","\n","                total_reward+=reward\n","\n","                if not done:\n","                    self.update_q_table(state, action, reward, next_state)\n","                else:\n","                    self.q_table[state, action] = 0\n","                    break\n","                state=next_state\n","\n","            reward_tracking.append(total_reward)\n","\n","            if end_eps_decay >= episode >= start_eps_decay:\n","                self.epsilon -= epsilon_decay_value\n","\n","        return reward_tracking"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sJoog7Ps7d7v"},"outputs":[],"source":["q_learner = Q_Agent(env.nstates, env.nactions)\n","q_rewards = q_learner.navigate(env, num_episodes=5000)\n","\n","plt.plot(running_mean(q_rewards,N=30))\n","plt.xlabel('Episode')\n","plt.ylabel('Reward Obtained Per Episode')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VC3IC21J7d7v"},"outputs":[],"source":["# Show values for each action\n","fig, ax = plt.subplots(1,4)\n","for i in range(4):\n","    ax[i].imshow(q_learner.q_table[:,i].reshape(n_rows,n_cols))\n","    ax[i].set_title(env.action_list[i])\n","plt.show()\n","\n","# show average across actions ~ V(s)\n","a = np.mean(q_learner.q_table, axis=1).reshape(n_rows,n_cols)\n","\n","f = plt.imshow(a)\n","plt.colorbar(f)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"IqWvGYRK7d7v"},"source":["### Compare MC, SARSA, and Q-Learning"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bRPSaGna7d7v"},"outputs":[],"source":["# make an environment we will use for these tabular agents \n","n_rows, n_cols = 10,10\n","step_penalization = -0.01\n","env = GridWorld(rows=n_rows,cols=n_cols,\n","                  obstacles=[(3,2),(3,3),(3,4),(4,2),(5,2),(3,5),(3,6),(3,7)], \n","                  rewards={(5,5):10},\n","                  terminals=[(5,5)],\n","                  step_penalization=step_penalization,\n","                  actionlist=['Down','Up','Right','Left'],\n","                  view_labels=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qTbsgoMS7d7v"},"outputs":[],"source":["## let's look at the reward function on the environment:\n","print('Reward Function in Environment:')\n","plot_reward_map(env)\n","\n","number_of_episodes = 5000\n","\n","## run the MC agent through the environment\n","mc_learner = MC_Agent(env.nstates, env.nactions)\n","print('Running MC Agent...'),\n","mc_rewards = mc_learner.navigate(env, num_episodes=number_of_episodes)\n","print('Done')\n","\n","## run the Q learning agent through the environment \n","q_learner = Q_Agent(env.nstates, env.nactions)\n","print('Running Q-Learning Agent...')\n","q_rewards = q_learner.navigate(env, num_episodes=number_of_episodes)\n","print('Done')\n","\n","## run the Q learning agent through the environment \n","sarsa_learner = SARSA_Agent(env.nstates, env.nactions)\n","print('Running SARSA Agent...')\n","sarsa_rewards = sarsa_learner.navigate(env, num_episodes=number_of_episodes)\n","print('Done')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CFBNL1Jg7d7v"},"outputs":[],"source":["## Plot agent performance for all agents\n","smoothing = 30\n","plt.plot(running_mean(mc_rewards,N=smoothing), label='MC')\n","plt.plot(running_mean(sarsa_rewards,N=smoothing), label='SARSA')\n","plt.plot(running_mean(q_rewards,N=smoothing), label='Q')\n","plt.legend(bbox_to_anchor=(1.25,0.95))\n","plt.xlabel('Episode')\n","plt.ylabel('Reward Obtained Per Episode')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xh-dTsyy7d7v"},"outputs":[],"source":["## Compare Values Learned by Agents \n","fig, ax = plt.subplots(3,env.nactions)\n","for i in range(env.nactions):\n","    ax[0,i].imshow(mc_learner.q_table[:,i].reshape(n_rows,n_cols))\n","    ax[1,i].imshow(q_learner.q_table[:,i].reshape(n_rows,n_cols))\n","    ax[2,i].imshow(sarsa_learner.q_table[:,i].reshape(n_rows,n_cols))\n","    ax[0,i].set_title(env.action_list[i])\n","\n","ax[0,0].set_ylabel('MC')\n","ax[1,0].set_ylabel('Q')\n","ax[2,0].set_ylabel('SARSA')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"S0Jm0j4b7d7v"},"source":["<b><span style='color:blue'> Try the above code in different environments, see how these agents are similar or different depending on the conditions they have to learn </span></b>"]},{"cell_type":"markdown","metadata":{"id":"gW3gFPtL7d7v"},"source":["# Mini Projects \n","\n","### Level 0: I want to get a better sense of the fundamentals\n","- Create your own variant of a gridworld task and solve it using the one (or all) of the learners we built during the turorial (MC/Q/SARSA agents)\n","    - Specify your own reward and transition functions (eg. what if you made a gridworld where the agent moved like Pacman?).  \n","    - Explore action-dependent reward functions. What if the agent can only get the reward by taking a specific action? Introduce a new 'special' action in addition to Up/Down/Left/Right. Remember to account for this in both your transition function and reward functions.\n","   \n","\n","### Level 1: I'm comfortable with fundamentals and want to try an extension of something we did already\n","- Explore making a new kind of environment. Solve the Knight's Tour task (suggestion: try both MC and Q learning, pick one to start and if you get it working, try the other).\n","<div><img src='https://images.chesscomfiles.com/uploads/v1/images_users/tiny_mce/PedroPinhata/phpT00lSF.png'></div>\n","  - The Knight's Tour problem is to move a knight around a chess board such that it visits every position on the 8x8 board once and only once. The knight moves in an L-shape. \n","  - For this problem you will need to write a new transition function for an 8x8 open field gridworld.\n","  - You will also need to determine how you want to shape the rewards for the task, as well as starting and stopping conditions. How does your agent get feedback?   \n","- Explore how the agents we looked at in a single environment behave across different environments. Compare and contrast the performance of a Q-Learner against a SARSA learner in different environments. \n","    - A SARSA agent updates the Q table in an ''on-policy'' fashion -- i.e. using the same policy as the one used to select actions. By contrast, Q learning is an ''off-policy'' algorithm, where the agent updates its Q values using a different policy than the one it used to select actions (the agent selects actions with an epsilon greedy policy, but updates Q values with a greedy policy). \n","    - Hint: Use the cliffworld environment -- why do these agents perform about the same in other environments, and what is it about this environment that highlights their differences?\n","\n","### Level 2: I want to explore some things we hinted at but didn't go into detail\n","- Project: Build a model-based agent (Dyna Q) to navigate a gridworld task of your design (use the Gridworld class from env_functions) \n","\n","<div><img src='https://publish-01.obsidian.md/access/fc5a2dd092cb06eefdd04ff307334d7a/_attachments/Tabular%20Dyna-Q.png'></div>\n","    - A Dyna agent relies on interative phases of model construction from sampling experience, and action/rewards collection from acting based on model-generated \n","- Project: Build a neural network to do Q learning (**) ~ DQN \n","\n","### Level 3: All this is childs play give me something hard\n","- Project: Solve a gridworld task with an actor-critic network agent -- use PyTorch\n","<div><img src='https://intellabs.github.io/coach/_images/ac.png'></div>\n","    - Construct a neural network (input / single hidden / output layers are fine, no need for deep architecture). What information do you want to include in the input? What information do you need in the actor & critic output layers? \n","    - What are appropriate loss functions?\n","    - Decide how to represent the state information to the network. What sort of semantic details are important to include?\n"]},{"cell_type":"markdown","metadata":{"id":"Hgnf9ou07d7v"},"source":["--------------------"]},{"cell_type":"markdown","metadata":{"id":"KwMl0boD7d7v"},"source":["# References \n","\n","- Sutton and Barto, An Introduction to Reinforcement Learning \n","- David Silver [Lecture Notes](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html)\n","- Omer Sezer RL Overview [Tutorials](https://github.com/omerbsezer/Reinforcement_learning_tutorial_with_demo) \n","\n","\n","## Some interesting papers\n","- Q-Learning: V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, et al. “Playing Atari with Deep Reinforcement Learning”. (2013).\n","- V. Mnih, K. Kavukcuoglu, D. Silver, et al. \"Human-level control through deep reinforcement learning\" (Nature-2015).\n","- Schulma et al. \"Proximal Policy Optimization Algorithms\"(2017).\n","- Hasselt et al. \"Deep Reinforcement Learning with Double Q-learning\" (2015).\n","- Schaul et al. \"Prioritized Experience Replay\" (2015).\n","- AlphaGo- Supervised learning + policy gradients + value functions + Monte Carlo tree search D. Silver, A. Huang, C. J.Maddison, A. Guez, L. Sifre, et al. “Mastering the game of Go with deep neural networks and tree search”. Nature (2016)."]},{"cell_type":"markdown","metadata":{"id":"vllAazJv7d7v"},"source":["Model-based RL: https://compneuro.neuromatch.io/tutorials/W3D4_ReinforcementLearning/student/W3D4_Tutorial4.html"]},{"cell_type":"markdown","metadata":{"id":"lXrLBLTO7d7w"},"source":["Links to neuroscience: https://compneuro.neuromatch.io/tutorials/W3D4_ReinforcementLearning/further_reading.html#links-to-neuroscience"]},{"cell_type":"markdown","metadata":{"id":"yEVrJiay7d7w"},"source":["Intro to Deep RL and DQN: https://deeplearning.neuromatch.io/tutorials/W3D4_BasicReinforcementLearning/student/W3D4_Tutorial5.html"]},{"cell_type":"markdown","metadata":{"id":"seE3AGZ67d7w"},"source":["PPO, DDPG, SAC: https://spinningup.openai.com/en/latest/algorithms/ppo.html"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EYTIoTD37d7w"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.1"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}